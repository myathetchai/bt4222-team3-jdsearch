{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This notebook builds a **GRU-based sequence encoder with attention** to generate a **dynamic user embedding** from each user's interaction history."],"metadata":{"id":"sPr63_caUlMV"}},{"cell_type":"markdown","source":["## Import Libraries and Load Dataset"],"metadata":{"id":"J1HK_Eem277S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvn7cm6z0e2p"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pyarrow.parquet as pq\n","import os, json, random\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","source":["# Mount\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dY4a_I2Z12Ye","executionInfo":{"status":"ok","timestamp":1762928983797,"user_tz":-480,"elapsed":20418,"user":{"displayName":"Shanice Chin","userId":"10806678144293196445"}},"outputId":"107bd1db-81d6-46b5-bb71-f12948cc7c1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Set random seeds for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A031zdS1DAH6","executionInfo":{"status":"ok","timestamp":1762928983806,"user_tz":-480,"elapsed":15,"user":{"displayName":"Shanice Chin","userId":"10806678144293196445"}},"outputId":"f734cd09-9552-40fb-c2b6-8e130a58300b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7c7580fe1770>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Data paths\n","DATA_PATH_MERGED = \"/content/drive/MyDrive/BT4222 Group 3/1. Data Preparation/Data/features_engineered/user_behavior_data_sampled_parsed_features_merged.parquet\"\n","SAVE_PATH = \"/content/drive/MyDrive/BT4222 Group 3/2. Models/Artifacts/gru_attention\"\n","os.makedirs(SAVE_PATH, exist_ok = True)"],"metadata":{"id":"UT-pn3kVN_kN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load merged df as a table\n","queries_table_with_engineered_merged = pq.read_table(DATA_PATH_MERGED)\n","queries_df = pd.DataFrame(queries_table_with_engineered_merged.to_pydict())"],"metadata":{"id":"sgS8egTR125N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["queries_df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EIixPNwx14jd","executionInfo":{"status":"ok","timestamp":1762929142354,"user_tz":-480,"elapsed":196,"user":{"displayName":"Shanice Chin","userId":"10806678144293196445"}},"outputId":"b9167773-2837-4079-feee-a16d09f049bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 173831 entries, 0 to 173830\n","Data columns (total 29 columns):\n"," #   Column                     Non-Null Count   Dtype  \n","---  ------                     --------------   -----  \n"," 0   query                      173831 non-null  object \n"," 1   candidate_wid_list         173831 non-null  object \n"," 2   candidate_label_list       173831 non-null  object \n"," 3   history_qry_list           173831 non-null  object \n"," 4   history_wid_list           173831 non-null  object \n"," 5   history_type_list          173831 non-null  object \n"," 6   history_time_list          173831 non-null  object \n"," 7   candidate_label_list_int   173831 non-null  object \n"," 8   history_type_list_ordinal  173831 non-null  object \n"," 9   query_list                 173831 non-null  object \n"," 10  history_qry_list_terms     173831 non-null  object \n"," 11  nunique_cats_1             173831 non-null  float64\n"," 12  nunique_cats_2             173831 non-null  float64\n"," 13  nunique_cats_3             173831 non-null  float64\n"," 14  nunique_cats_4             173831 non-null  float64\n"," 15  avg_log_cat_1_popularity   169316 non-null  float64\n"," 16  history_len                173831 non-null  int64  \n"," 17  avg_interaction_time_gap   164397 non-null  float64\n"," 18  std_time_gap               159877 non-null  float64\n"," 19  min_gap                    159877 non-null  float64\n"," 20  max_gap                    159877 non-null  float64\n"," 21  total_active_days          159877 non-null  float64\n"," 22  click_ratio                169316 non-null  float64\n"," 23  follow_ratio               169316 non-null  float64\n"," 24  cart_ratio                 169316 non-null  float64\n"," 25  order_ratio                169316 non-null  float64\n"," 26  user_ratio_cluster         173831 non-null  int64  \n"," 27  total_time_decay           173831 non-null  float64\n"," 28  avg_time_decay             173831 non-null  float64\n","dtypes: float64(16), int64(2), object(11)\n","memory usage: 38.5+ MB\n"]}]},{"cell_type":"code","source":["# Use GPU if available, else fall back to CPU\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"nb02JumEDO_Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Helpers\n","\n","This section defines small helper utilities used throughout the notebook.\n","\n","- `TIME_BINS` specifies discrete time buckets in days\n","- `bucketize_gaps(gaps)` converts raw time gaps to bucket indices\n","- `clip_recent(seq, max_len)` keeps only the most recent `max_len` interactions"],"metadata":{"id":"5YYAykml2tUA"}},{"cell_type":"code","source":["# Time gap bins in days (adjust as needed)\n","TIME_BINS = [0, 1, 3, 7, 14, 30, 60, 90, 180, 365, 1000]\n","\n","# Convert each time gap into a bucket index based on TIME_BINS\n","def bucketize_gaps(gaps):\n","  if gaps is None:\n","    return []\n","  out = []\n","  for gap in gaps:\n","    try:\n","      out.append(int(np.digitize(float(gap), TIME_BINS)))\n","    except Exception:\n","      out.append(int(np.digitize(0.0, TIME_BINS)))\n","  return out\n","\n","# Keep only the most recent max_len steps in sequence\n","# - Ensure that sequence length is manageable for the model\n","def clip_recent(seq, max_len):\n","  if max_len is None:\n","    return seq\n","  if len(seq) > max_len:\n","    return seq[-max_len:]\n","  else:\n","    return seq"],"metadata":{"id":"Rnoat8R8cl1b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build vocabularies for embedding\n","\n","Item vocabulary: `wid2idx`\n","- Maps each unique item ID from `history_wid_list` to an integer index\n","- Index 0 is reserved for padding\n","- `N_ITEMS` gives the vocabulary size for the item embedding layer\n","\n","Interaction type vocabulary: `type2idx`\n","- Maps each unique interaction type from `history_type_list_ordinal` to an integer index\n","- Index 0 is reserved for padding\n","- `N_TYPES` gives the vocabulary size for the type embedding layer\n","\n","Time buckets:\n","- `TIME_BINS` (from the helper section) defines the discrete time gap buckets\n","- `N_TIME` gives the vocabulary size for the time embedding layer"],"metadata":{"id":"54EiDL7qMTK7"}},{"cell_type":"code","source":["# Build wid2idx: map each unique wid to an idx  (0=PAD, idx starts from 1)\n","all_wids = set(w for seq in queries_df[\"history_wid_list\"] for w in (seq or []))\n","wid2idx = {w: idx+1 for idx, w in enumerate(sorted(all_wids))}\n","N_ITEMS = len(wid2idx) + 1\n","\n","# Build type2idx: map each unique interaction type to an idx  (0=PAD, idx starts from 1)\n","type_set = set(t for seq in queries_df[\"history_type_list_ordinal\"] for t in (seq or []))\n","type2idx = {t: idx+1 for idx, t in enumerate(sorted(type_set))}\n","N_TYPES = len(type2idx) + 1\n","\n","# No. of possible time bucket idx  (0=PAD + bins)\n","N_TIME = len(TIME_BINS) + 1\n","\n","# Save vocab artifacts for reuse\n","with open(os.path.join(SAVE_PATH, \"wid2idx.json\"), \"w\") as f: json.dump(wid2idx, f)\n","with open(os.path.join(SAVE_PATH, \"type2idx.json\"), \"w\") as f: json.dump(type2idx, f)\n","with open(os.path.join(SAVE_PATH, \"time_bins.json\"), \"w\") as f: json.dump(TIME_BINS, f)"],"metadata":{"id":"O2iY3Bl7MVem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset & Collate function"],"metadata":{"id":"5r_G-hQqEoZr"}},{"cell_type":"markdown","source":["The `HistoryDataset` class converts each user's sequential interaction history into a numerical format suitable for embedding and GRU encoding.\n","\n","**Design choices**:\n","1. **History truncation**\n","\n","    `clip_recent` keeps only the last `MAX_LEN` interactions to focus on recent user behaviour and avoid excessive memory consumption\n","\n","2. **Empty history handling**\n","\n","    Users with no recorded history are represented by a single padded step and `true_len = 0`, ensuring model stability and avoiding shape mismatches\n","\n","3. **Index mapping**\n","\n","    - `history_wid_list`: Map each unique wid to item indices (`wid2idx`)\n","    - `history_type_list_ordinal`: Map each interaction type to interaction-type indices (`type2idx`)\n","    - `history_time_list`: Map raw time gaps to time-bucket indices (`bucketize_gaps`)\n","\n","4. **Output**\n","\n","    Returns a dictionary of PyTorch tensors for each sequence:\n","    - `\"wid\"` – item index sequence  \n","    - `\"type\"` – interaction type index sequence  \n","    - `\"time\"` – time-bucket index sequence  \n","    - `\"true_len\"` – actual unpadded sequence length  \n","    - `\"row_id\"` – original dataframe row reference"],"metadata":{"id":"ajDveL-D1XaV"}},{"cell_type":"code","source":["# Max no. of steps to keep in a user's history\n","MAX_LEN = 100\n","\n","class HistoryDataset(Dataset):\n","  def __init__(self, df, wid2idx, type2idx, max_len=MAX_LEN):\n","    self.df = df\n","    self.wid2idx = wid2idx\n","    self.type2idx = type2idx\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    return len(self.df)\n","\n","  def _map_seq(self, seq, mapping):\n","    # convert each token in sequence to idx using mapping.\n","    # if token not found, map to 0 (PAD)\n","    return [mapping.get(tok, 0) for tok in (seq or [])]\n","\n","  def __getitem__(self, idx):\n","    # get 1 row by idx\n","    row = self.df.iloc[idx]\n","\n","    # extract history lists and clip to most recent max_len steps\n","    wid_seq = clip_recent(row[\"history_wid_list\"] or [], self.max_len)\n","    type_seq = clip_recent(row[\"history_type_list_ordinal\"] or [], self.max_len)\n","    time_seq = clip_recent(row[\"history_time_list\"] or [], self.max_len)\n","\n","    # empty history list\n","    if len(wid_seq) == 0:\n","      return {\n","          \"wid\": torch.tensor([0], dtype=torch.long),\n","          \"type\": torch.tensor([0], dtype=torch.long),\n","          \"time\": torch.tensor([0], dtype=torch.long),\n","          \"true_len\": torch.tensor(0, dtype=torch.long),\n","          \"row_id\": idx\n","      }\n","\n","    # map wid and type to idx\n","    wid_idx = torch.tensor(self._map_seq(wid_seq, self.wid2idx), dtype = torch.long)\n","    type_idx = torch.tensor(self._map_seq(type_seq, self.type2idx), dtype = torch.long)\n","\n","    # convert raw time gaps into time-bucket indices\n","    time_idx = torch.tensor(bucketize_gaps(time_seq), dtype = torch.long)\n","\n","    return {\n","        \"wid\": wid_idx,\n","        \"type\": type_idx,\n","        \"time\": time_idx,\n","        \"true_len\": torch.tensor(len(wid_idx), dtype = torch.long),  #before padding\n","        \"row_id\": idx\n","    }"],"metadata":{"id":"4DuW8IpeP7Sp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Users have **different sequence lengths**, but GRUs expects tensors of the same length within a batch.\n","\n","We therefore pad sequences, sort by length (for packed sequences), and keep an index to restore the original order after the forward pass."],"metadata":{"id":"TFeYe-0uzpVL"}},{"cell_type":"markdown","source":["The `collate_fn(batch)` prepares a batch of variable-length sequences for the GRU.\n","\n","**Steps:**\n","1. **Extract fields** from each sample in `batch`: `\"wid\"`, `\"type\"`, `\"time\"`, `\"true_len\"`, `\"row_id\"`\n","2. **Pad sequences** to equal length using `pad_sequence` with PAD index = 0\n","3. **Sort** by length descending to support `pack_padded_sequence`\n","4. **Keep restore index** so that outputs can be re-ordered back to the original batch order\n","\n","**Outputs**\n","- `\"wid\"`, `\"type\"`, `\"time\"` – padded index tensors, sorted by length\n","- `\"lens\"` – sorted true lengths\n","- `\"restore\"` – indices to invert the sort\n","- `\"row_ids_sorted\"` – original dataframe row ids, in the sorted order"],"metadata":{"id":"ulzBSs8E8eIP"}},{"cell_type":"code","source":["def collate_fn(batch):\n","  wids = [b[\"wid\"] for b in batch]\n","  types = [b[\"type\"] for b in batch]\n","  times = [b[\"time\"] for b in batch]\n","  lens = torch.stack([b[\"true_len\"] for b in batch], dim = 0)\n","  rows = torch.tensor([b[\"row_id\"] for b in batch], dtype = torch.long)\n","\n","  # Pad sequences to the same length (using 0 as PAD)\n","  wids_p = pad_sequence(wids, batch_first = True, padding_value = 0)\n","  types_p = pad_sequence(types, batch_first = True, padding_value = 0)\n","  times_p = pad_sequence(times, batch_first = True, padding_value = 0)\n","\n","  # Sort sequences by length in descending order\n","  lens_sorted, idx = torch.sort(lens, descending = True)\n","\n","  # Compute inverse idx so we can restore original batch later on\n","  restore = torch.argsort(idx)\n","\n","  return {\n","      \"wid\": wids_p[idx],\n","      \"type\": types_p[idx],\n","      \"time\": times_p[idx],\n","      \"lens\": lens_sorted,\n","      \"restore\": restore,\n","      \"row_ids_sorted\": rows[idx]\n","  }"],"metadata":{"id":"hks52pWIhUle"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now wrap the data into the `HistoryDataset` and `DataLoader` so that user histories can be efficiently processed in batches."],"metadata":{"id":"5QF8bHMo5L7A"}},{"cell_type":"code","source":["# Build dataset and dataloader\n","dataset = HistoryDataset(queries_df, wid2idx, type2idx, max_len=MAX_LEN)\n","dataloader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=0,\n","                        collate_fn=collate_fn, pin_memory=(device==\"cuda\"))"],"metadata":{"id":"abFbq-g0c95f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating the Embeddings\n","\n","**Goal**: Generate a dense, fixed-size embedding for each user that captures their sequential interaction patterns with items.\n","\n","**Design Choices**\n","\n","1. **GRU (Gated Recurrent Unit)**  \n","   - Handles sequences of varying lengths.  \n","   - Captures temporal dependencies, i.e. how past actions influence future behavior.\n","   - Chosen over LSTM for similar performance but lower computational cost.\n","\n","2. **Attention Layer**  \n","   - Aggregates GRU outputs into a single weighted embedding.  \n","   - Allows the model to focus on more important interactions instead of treating all steps equally.\n","\n","3. **Embedding Sum**  \n","   - Each user interaction is represented by three components: product ID, interaction type, and time gap bucket.  \n","   - We sum their embeddings to get a combined representation for each time step.\n","\n","4. **Empty History Handling**  \n","   - Users with no historical interactions are represented by a padded step.  \n","   - Ensures stability and prevents shape mismatches during batching.\n","\n","5. **Batch Processing with Padding and Packing**  \n","   - `pad_sequence` ensures all sequences in a batch have the same length.  \n","   - `pack_padded_sequence` allows GRU to skip padded steps efficiently, preserving variable-length information.\n","\n","6. **Precomputation for Downstream Tasks**  \n","   - Embeddings are computed once and stored, allowing fast usage in downstream tasks without recomputing each time."],"metadata":{"id":"pM4WVnMcbITR"}},{"cell_type":"markdown","source":["The following cell defines the GRU-based sequence encoer architecture used to generate user embeddings."],"metadata":{"id":"DaFxyZoRKWIl"}},{"cell_type":"code","source":["class SeqEncoder(nn.Module):\n","  def __init__(self, n_items, n_types, n_time, emb_dim=64, dropout=0.1):\n","    super().__init__()\n","\n","    # Embedding layers\n","    self.item_emb = nn.Embedding(n_items, emb_dim, padding_idx=0)\n","    self.type_emb = nn.Embedding(n_types, emb_dim, padding_idx=0)\n","    self.time_emb = nn.Embedding(n_time, emb_dim, padding_idx=0)\n","\n","    # GRU reads the sequence of embedded steps\n","    self.gru = nn.GRU(input_size=emb_dim, hidden_size=emb_dim, batch_first=True)\n","\n","    # Attention layer\n","    self.attn = nn.Linear(emb_dim, 1)\n","\n","    # Dropout layer for regularisation\n","    self.dropout = nn.Dropout(dropout)\n","\n","\n","  def forward(self, wid, type_, time_, lens):\n","    # Convert ids to embeddings, Sum embeddings at each timestep\n","    x = self.item_emb(wid) + self.type_emb(type_) + self.time_emb(time_)\n","\n","    # Pack padded batch\n","    lens_cpu = lens.cpu().clamp_min(1)\n","    packed = pack_padded_sequence(x, lengths=lens_cpu, batch_first=True, enforce_sorted=False)\n","    packed_out, _ = self.gru(packed)\n","    out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n","\n","    # Mask out padding positions\n","    mask = (wid != 0).float()\n","    attn_scores = self.attn(out).squeeze(-1)\n","    attn_scores = attn_scores.masked_fill(mask == 0, -1e9)  #ignore PADs\n","    attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)\n","\n","    # Weighted sum of GRU outputs\n","    context = torch.sum(attn_weights * out, dim=1)\n","    return self.dropout(context)"],"metadata":{"id":"WSQhQUeenQXO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we use the model in evaluation mode to compute user embeddings in batches."],"metadata":{"id":"fKSzHQ6nKeGJ"}},{"cell_type":"code","source":["model = SeqEncoder(N_ITEMS, N_TYPES, N_TIME).to(device)\n","model.eval()\n","\n","user_dynamic_chunks = []\n","row_chunks = []\n","\n","with torch.no_grad():\n","  for batch in dataloader:\n","    wid = batch[\"wid\"].to(device)\n","    typ = batch[\"type\"].to(device)\n","    time = batch[\"time\"].to(device)\n","    lens = batch[\"lens\"].to(device)\n","    restore = batch[\"restore\"].to(device)\n","\n","    # Forward pass: Get user embeddings for batch\n","    u = model(wid, typ, time, lens)\n","\n","    # Restore original raw order within batch\n","    u = u[restore]\n","\n","    # Get original row_ids in correct order\n","    rows = batch[\"row_ids_sorted\"][restore].cpu().numpy()\n","\n","    user_dynamic_chunks.append(u.cpu().numpy())\n","    row_chunks.append(rows)\n","\n","# Concatenate all batches into 1 array\n","user_dynamic_embs = np.concatenate(user_dynamic_chunks, axis=0)\n","row_ids = np.concatenate(row_chunks, axis=0)"],"metadata":{"id":"FmnMamESofqB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Export data"],"metadata":{"id":"MHw7Yj9F2aYS"}},{"cell_type":"code","source":["# Save embeddings\n","np.save(os.path.join(SAVE_PATH, \"user_dynamic_embs.npy\"), user_dynamic_embs)"],"metadata":{"id":"nImISqgh_XAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add embeddings as a new column in a copy of dataframe\n","df_out = queries_df.copy()\n","df_out[\"user_dynamic_embs\"] = list(user_dynamic_embs)"],"metadata":{"id":"U5SMo8Tdz0dl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save dataframe with embeddings to parquet\n","df_out.to_parquet(os.path.join(SAVE_PATH, \"user_dynamic_embs.parquet\"))"],"metadata":{"id":"lLXoIaFb2bRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save state_dict with N_ITEMS, N_TYPES, N_TIME\n","checkpoint = {\n","    'state_dict': model.state_dict(),\n","    'n_items': N_ITEMS,\n","    'n_types': N_TYPES,\n","    'n_time': N_TIME\n","}\n","torch.save(checkpoint, os.path.join(SAVE_PATH, \"seq_encoder_ckpt.pt\"))"],"metadata":{"id":"fI7-Ak7c1ek5"},"execution_count":null,"outputs":[]}]}